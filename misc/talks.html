<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Research talks on theoretical computer science and machine learning</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="../assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="../assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="../../assets/css/ie8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="../../assets/css/ie9.css" /><![endif]-->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
 <link rel="shortcut icon" href="../images/iitgoa_logo_wx.png" type="image/x-icon"/>
	</head>
	<body>


		<!-- Nav -->
			<nav id="nav">
				<ul class="container">
					<li><a href="../index.html#home" class="scrolly">Home</a></li>
					<li><a href="../index.html#research" class="scrolly">Research</a></li>
					<li><a href="../index.html#teaching" class="scrolly">Teaching</a></li>
					<li><a href="../index.html#publications" class="scrolly">Publications</a></li>
					<li><a href="../index.html#links" class="scrolly">Links</a></li>
				</ul>
			</nav>

		<!-- Talks -->
			<div class="wrapper style1 first">
				<article class="container" id="network">
					<header>
						<h2>Research Seminar</h2>
					</header>
					<div align="left">
						<strong>Time: </strong> Monday's 3 pm - 4.30 pm 
                        
                        <br><strong>Link: </strong> <a href="https://meet.google.com/pdn-xsye-bsq">
							https://meet.google.com/pdn-xsye-bsq </a> 

						<br><strong>Resources: </strong> Use iitgoa email id for accessing video links to the talks. <br> <br>
                        
                    
						<strong>Talks</strong><br>
						<table>
								<thead align="left">
									<tr>
											<th> # </th>
											<th>Date</th>
											<th>Talk details</th>
											<th>Resource</th>
									</tr>
								</thead>
								<tbody>
                                    <tr align="left">
                                        <th> </th> <td></td>
                                        <td> No talk scheduled for 11/10 and 18/10 </td>
                                        <td> </td>
                                    </tr>                                    
                                    <tr align="left">
                                        <th> 17 </th> 
                                        <td> 4/10 </td>
                                        <td> <b> Title: </b>  Equivalence checking methods for proving equivalence for scalar and array-handling programs <br/>
                                            <b> Speaker: </b> Sudakshina Dutta <br/>
                                            <b> Abstract: </b> Program equivalence is the problem of theoretically proving that two programs are functionally equal. In this presentation, a method of proving equivalence for scalar-handling programs and a method of proving equivalence for array-handling programs will be presented with simple examples. Also, the recent advances in this area will be briefly discussed.
                                        <td>  </td>
                                    </tr>
                                    <tr align="left">
                                        <th> 16 </th> 
                                        <td> 27/9 </td>
                                        <td> <b> Title: </b> Estimation of finite mixtures of Gaussians via Bayesian sampling <br/>
                                            <b> Speaker: </b> Clint P. George <br/>
                                            <b> Abstract: </b> In this talk, I review the hierarchical model of the Bayesian mixture of Gaussians. I then present the popular Markov chain Monte Carlo method, Gibbs sampler, to generate samples from the posterior of the model. We also look at a toy example that illustrates the generative process of the model and the working of the Gibbs sampler. <br/>
                                            Reference: <a href="https://www.jstor.org/stable/2345907"> Estimation of Finite Mixture Distributions through Bayesian Sampling by Jean Diebolt and Christian P. Robert </a>
                                        <td> <a href="https://drive.google.com/file/d/1CRHZ7mkPu84EgEy5Jps8XkFaU682YD1g/view"> video </a> </td>
                                    </tr>
                                    <tr align="left">
                                        <th> 15 </th> 
                                        <td> 20/9 </td>
                                        <td> <b> Title: </b> Learning regular sets from queries and counterexamples <br/>
                                            <b> Speaker: </b> Prince Mathew <br/>
                                            <b> Abstract: </b> In this talk, we address the problem of learning an unknown regular language using <i>membership</i> and <i>equivalence</i> queries. We assume that this regular language is presented by a <i>minimally adequate teacher</i> who can determine the membership of a word in that language. The teacher can also check a given language is not equal to the unknown regular language and provide a counterexample. We will discuss the L* algorithm, which correctly learns any regular language from the teacher using membership and equivalence queries in time polynomial in the number of states of the minimal DFA recognising the regular language and the length of the longest counterexample provided by the teacher.

                                            <br/> Reference: <a href="https://www.sciencedirect.com/science/article/pii/0890540187900526"> Angluin's seminal paper </a>
                                        <td> <a href="https://drive.google.com/file/d/1q-JUAgaXcMIxTvmfkdx02pnu8_YTm3cD/view"> video </a> </td>
                                    </tr>
                                    <tr align="left">
                                        <th> 14 </th> 
                                        <td> 13/9 </td>
                                        <td> <b> Title: </b> Mixture models and inference II <br/>
                                            <b> Speaker: </b> Clint George continues <br/> 
                                            <a href="Bayesian-mixture-models-I"> Clint's hand-notes </a>
                                        <td><a href="https://drive.google.com/file/d/10uCdrRSSSvSgEAXE4gNZiUAIQlE7g8mw/view?usp=sharing"> video </a> <br/>
                                     </td>
                                    </tr>
                                    <tr align="left">
                                        <th> 13 </th> 
                                        <td> 6/9 </td>
                                        <td> <b> Title: </b> Mixture models and inference I <br/>
                                            <b> Speaker: </b> Clint George <br/>
                                            <b> Abstract: </b> This talk looks at some popular mathematical models such as k-means clustering and probabilistic mixtures of Gaussians for grouped, continuous data. We see that we can derive an efficient iterative scheme for k-means clustering via error minimization. We set up a Bayesian approach for the mixture of Gaussians, but we will see that exact posterior inference is intractable. We thus discuss a posterior inference scheme based on Markov chain Monte Carlo methods. <br/>
                                            <a href="Bayesian-mixture-models-I"> Clint's hand-notes </a>
                                        <td> <a href="https://drive.google.com/file/d/18l4x3mYGj9bdgSbCKcadpK-MUUVR28jG/view"> video </a> </td>
                                    </tr>
                                    <tr align="left">
                                        <th> </th> <td></td>
                                        <td> No talk scheduled for 23/8 and 30/8 </td>
                                        <td> </td>
                                    </tr>
                                    <tr align="left">
                                        <th> 12 </th>
                                        <td> 16/8 </td>
                                        <td> <b> Title: </b> Isolation lemma <br/>
                                            <b> Speaker: </b> Sreejith A. V. <br/>
                                            <b> Abstract: </b> In this talk, we look at the Isolation lemma of Mulmuley, Vazirani and Vazirani. The lemma helps one randomly reduce number of solutions of an NP problem to one. It has various applications in computational complexity and randomized algorithms. <br/>
                                            See the <a href="http://www.cs.tau.ac.il/~amnon/Classes/2017-BPP/Lectures/Lecture14a.pdf"> lecture notes by Amnon Ta-Shma and Dean Doron </a>
                                        </td>
                                        <td>
                                        <a href="https://drive.google.com/file/d/1-Xt0iThQyIXn3Kclqh9fsRF6dskQrn6j/view"> video </a>
                                        </td>
                                    </tr>
									<tr align="left">
										<th> 11 </th>
										<td> 9/8 </td>
										<td> <b>Title: </b>Vapnik–Chervonenkis theory VI <br />
											<b>Speaker: </b> Satyanath Bhat <br />
										</td>
										<td>
                                        <a href="https://drive.google.com/file/d/1NTV1HGrNkkjZHChs0mBXMpsMlFPvl7BP/view">
                                        video </a> <br>
                                        <a href="erm.pdf"> scribe </a>
										</td>
									</tr>                                    
									<tr align="left">
										<th> 10 </th>
										<td> 2/8 </td>
										<td> <b>Title: </b>Vapnik–Chervonenkis theory V <br />
											<b>Speaker: </b> Satyanath Bhat <br />
										</td>
										<td>
                                        <a href="https://drive.google.com/file/u/1/d/1qpCeHmPGXcwpkYnHfuv2zWdlroe_lqm-/view">
                                        video </a> <br>
                                        <a href="erm.pdf"> scribe </a>
										</td>
									</tr>                                    
									<tr align="left">
										<th> 9 </th>
										<td> 26/7 </td>
										<td> <b>Title: </b>Vapnik–Chervonenkis theory IV <br />
											<b>Speaker: </b> Satyanath Bhat <br />
										</td>
										<td>
											<a href="https://drive.google.com/file/d/1mimivx0lPl_rA1OhpGrxQFg3C2bku7O7/view"> video </a> <br>
											<a href="erm.pdf"> scribe </a>                                            
										</td>
									</tr>
									<tr align="left">
										<th> 8 </th>
										<td> 19/7 </td>
										<td> <b>Title: </b>Vapnik–Chervonenkis theory III <br />
											<b>Speaker: </b> Satyanath Bhat <br />
										</td>
										<td>
											<a href="https://drive.google.com/file/d/1GDAftUFG6NV5PhTaGZ_4c-fr4--T7a-S/view?usp=sharing"> video </a> <br>
											<a href="erm.pdf"> scribe </a>
										</td>
									</tr>
									<tr align="left">
										<th> 7 </th>
										<td> 12/7 </td>
										<td> <b>Title: </b>Vapnik–Chervonenkis theory II <br />
											<b>Speaker: </b> Satyanath Bhat <br />
										</td>
										<td>
											<a href="https://drive.google.com/file/d/18xwzquiAro6Lkq0_YENocYQ2rs-ukCWq/view"> video </a> <br>
											<a href="erm.pdf"> scribe </a>
										</td>
									</tr>
									<tr align = "left">
										<th> 6 </th>
										<td> 5/7 </td>
										<td> <b>Title: </b>Vapnik–Chervonenkis theory I <br/>
											 <b>Speaker: </b> Satyanath Bhat <br/>
											<b>Abstract: </b> Vapnik–Chervonenkis theory provides a characterization of when a learning model is feasible. 
											In this series, we will go through the VC theory systematically. We will assume basic probability. 
											For example, see chap 1-3 from Ross, Sheldon M.
											Introduction to probability models (9th edition). Besides this no further background is assumed. <br>

											Simulation of polynomial noiseless demo: 
											<a href="https://colab.research.google.com/drive/1bQU8VxH5WQ-qVeyeWvXpvf0Ixs1ArNNs?usp=sharing"> colab link </a>  and  
											<a href="PolynomialNoiselessDemo.pdf"> pdf </a>
										</td>
										<td> 
											<a href="https://drive.google.com/file/d/1uNaT6F8wFDKC28lFQE2SiFaW9DlsBOK2/view?usp=sharing"> video </a> <br>
											<a href="erm.pdf">notes</a>
										</td>
									</tr>
                                    <tr align="left">
                                        <th> 5 </th>
                                        <td> 28/6 </td>
                                        <td> <b>Title:</b> An algebraic characterisation of first-order logic with neighbour <br />
                                        <b>Speaker:</b> Amaldev Manuel <br />
                                        <b>Abstract:</b> The aim of the talk is to shed some light on the basic concerns of the logic-algebra discipline. 
                                        As an example I will take the logic FO(N) and describe an algebraic characterisation for it. 
                                        We will only assume familiarity with regular languages. 
                                        </td>
                                        <td> <a href="https://drive.google.com/file/d/15Thm9HJnKsJHUUUCaalzcIRZ0cl9VeSI/view?usp=sharing"> video </a> <br/>
                                        <a href="fon.pdf">notes</a> 
                                        </td>
                                    </tr>
                                    <tr align ="left">
                                        <th> 4 </th>
                                        <td> 21/6 </td>
                                        <td> <b>Title:</b> Hypersafety Verification and Programming Assignment Evaluation <br />
                                        <b>Speaker:</b> Kumar Madhukar (TCS Research) <br />
                                            <b>Abstract:</b> In this talk, I will relate the problem of hypersafety verification to that of evaluating programming assignment submitted by a
                                                student (wrt a reference implementation provided by the teacher). I will present Ron et al.'s self-composition technique for
                                                hypersafety verification, explain its applicability for assignment evaluation, and propose an enhancement that makes their technique fully automatic. <br />
                                            <b>Reference:</b> Property Directed Self Composition by Ron et. al.
                                          </td>
                                        <td> <a href=https://drive.google.com/file/d/1HYXqtAnwBIgSCYmAC4QiVfBCShf_6zxP/view?usp=sharing"> video </a> <br />
                                        <a href="madhukar_talk.pdf">talk pdf</a> 
                                        </td>
                                    </tr>
									<tr align="left">
										<th> 3 </th>
										<td> 14/6 <br /> 18/6 </td>
										<td> <b>Title:</b> The ellipsoid method for solving linear programming <br>
											<b>Speaker:</b> Sreejith A V <br>
											<b>Abstract:</b> In this talk, we look at the ellipsoid method for solving linear programming.
											This is a polynomial time algorithm. <br />
                                            <b>Reference:</b>   1) Introduction to Linear Optimization by D. Bertsimas and J.N. Tsitsiklis, Chapter 8 <br />
                                                                2) Combinatorial Optimization by Papadimitriou and Steiglitz, Chapter 8
											<br>
										</td>
										<td> <a href="https://drive.google.com/file/d/1C8WGAGZn0wp4sbo9cD21jr0Eo66kix6c/view?usp=sharing">
												video 1 </a> <br>
                                            <a href=https://drive.google.com/file/d/11TyClEqBADEa64FuYGHVsEhm5AUs6Gkd/view?usp=sharing"> video 2 </a>
											<a href="ellipsoid.pdf">notes</a>
                                        </td>
									</tr>
									<tr align="left">
										<th> 2 </th>
										<td> 7/6 </td>
										<td> <b>Title:</b> Exploiting the spatial dimension of big data jobs for efficient cluster job scheduling <br>
											<b>Speaker:</b> Akshay Jajoo <br>
										</td>
										<td> </td>
									</tr>
									<tr align="left">
										<th> 1 </th>
										<td> 1/6 </td>
										<td> <b>Title:</b> Approximation Guarantee for the Max-Cut Problem using Semi-Definite Programming <br>
											<b>Speaker:</b> Divya Padmanabhan <br>
											<b>Abstract:</b> The Max-Cut problem is a well studied graph theoretic problem where one is interested in finding the cut of the largest
											value in a given graph. While the problem is NP-hard, it finds several applications in clustering, VLSI design, and more
											generally in network domains. For many years, various researchers had worked with different kinds of approximation
											techniques but could not get beyond a guarantee of 0.5. In 1995, Goemans and Williamson proposed an approximation using
											convex optimization (particularly with semi-definite programming) and were able to obtain a highly improved guarantee of
											0.878. This is the best known approximation guarantee for the max-cut problem today. In this talk, I will briefly
											introduce semi-definite programming for those new to the topic and provide a proof of the Goemans-Williamson result.<br>
                                            <b>Reference:</b> Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems Using Semidefinite Programming by Goemans and Williamson.
                                            <a href="http://www-math.mit.edu/~goemans/PAPERS/maxcut-jacm.pdf">pdf</a>
										</td>
										<td> <a href="https://drive.google.com/file/d/1mtLXbm4TqSPl_ZuZcp5PFwxbCSJ7cYVi/view?usp=sharing"> video </a> <br>
											<a href="maxcut.pdf">notes</a>
										</td>
									</tr>
                            </tbody>
                        </table>
					</div>
				</article>
			</div>

		<!-- Scripts -->
			<script src="../assets/js/jquery.min.js"></script>
			<script src="../assets/js/jquery.scrolly.min.js"></script>
			<script src="../assets/js/skel.min.js"></script>
			<script src="../assets/js/skel-viewport.min.js"></script>
			<script src="../assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="../../assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="../assets/js/main.js"></script>

	</body>
</html>
